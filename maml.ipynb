{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('mnist', download=False, train=True, transform=ToTensor())\n",
    "test_dataset = datasets.MNIST('mnist', download=False, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, layer_sizes=[128,128]):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer1_size = layer_sizes[0]\n",
    "        self.layer2_size = layer_sizes[1]\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, layer_sizes[0])\n",
    "        self.layer2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.output_layer = nn.Linear(layer_sizes[1], output_size)\n",
    "        \n",
    "    def forward(self, input_batch, weights=None):        \n",
    "        x = F.relu(F.linear(input_batch,weights['layer1.weight'],weights['layer1.bias']))\n",
    "        x = F.relu(F.linear(x,weights['layer2.weight'],weights['layer2.bias']))\n",
    "        output = F.linear(x,weights['output_layer.weight'],weights['output_layer.bias'])\n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SimpleMLP(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mlp(Variable(torch.randn(64,784)), base_weights)\n",
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on 0, 2, 4, 6, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta train on 1, 3, 5, 7, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset.train_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset_np.reshape((train_dataset_np.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels  = np.array(train_dataset.train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_minibatches(data, labels, task, train=True):\n",
    "    task_indices = np.where(labels == task)[0]\n",
    "    task_data = data[task_indices]\n",
    "    n = len(task_data)\n",
    "    if train:\n",
    "        index_ = int(0.8 * n)\n",
    "        train_data = task_data[0:index_]\n",
    "        train_labels = labels[0:index_]\n",
    "        rand_indices = np.random.randint(0,index_-1,64)\n",
    "        train_data = train_data[rand_indices]\n",
    "        train_labels = [task] * len(train_data)\n",
    "        return Variable(torch.FloatTensor(train_data.tolist())), Variable(torch.LongTensor(train_labels), requires_grad = False)\n",
    "    else:\n",
    "        index_ = int(0.8 * n)\n",
    "        rand_indices = np.random.randint(index_,len(task_data),64)\n",
    "        test_data = task_data[rand_indices]\n",
    "        test_labels = [task] * len(test_data)\n",
    "        return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen_minibatches(train_dataset_np, train_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_TASKS = [0,2,4,6,8]\n",
    "ALPHA = 0.01\n",
    "BETA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4457\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4242\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.1962\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.2197\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())\n",
    "task_weights = []\n",
    "for task in TRAIN_TASKS:\n",
    "    train_img_batch, train_labels_batch = gen_minibatches(train_dataset_np, train_labels, task)\n",
    "    output = mlp(train_img_batch, base_weights)\n",
    "    loss = loss_function(output, train_labels_batch)\n",
    "    print loss\n",
    "    grad_params = grad(loss, mlp.parameters(), create_graph=True)\n",
    "    task_weights.append(OrderedDict((name, param - ALPHA*grad) for ((name, param), grad) in zip(base_weights.items(), grad_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outer loss = all_task_loss\n",
    "# outer gradient = summation_on_inner_gradients\n",
    "# outer update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy = OrderedDict({k: sum(d[k] for d in task_weights) for k in task_weights[0].keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
