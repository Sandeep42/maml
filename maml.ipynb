{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('mnist', download=False, train=True, transform=ToTensor())\n",
    "test_dataset = datasets.MNIST('mnist', download=False, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, layer_sizes=[128,128]):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer1_size = layer_sizes[0]\n",
    "        self.layer2_size = layer_sizes[1]\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, layer_sizes[0])\n",
    "        self.layer2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.output_layer = nn.Linear(layer_sizes[1], output_size)\n",
    "        \n",
    "    def forward(self, input_batch, weights=None):        \n",
    "        x = F.relu(F.linear(input_batch,weights['layer1.weight'],weights['layer1.bias']))\n",
    "        x = F.relu(F.linear(x,weights['layer2.weight'],weights['layer2.bias']))\n",
    "        output = F.linear(x,weights['output_layer.weight'],weights['output_layer.bias'])\n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SimpleMLP(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mlp(Variable(torch.randn(64,784)), base_weights)\n",
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on 0, 2, 4, 6, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta train on 1, 3, 5, 7, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset.train_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset_np.reshape((train_dataset_np.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels  = np.array(train_dataset.train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_minibatches(data, labels, task, train=True):\n",
    "    task_indices = np.where(labels == task)[0]\n",
    "    task_data = data[task_indices]\n",
    "    n = len(task_data)\n",
    "    if train:\n",
    "        index_ = int(0.8 * n)\n",
    "        train_data = task_data[0:index_]\n",
    "        train_labels = labels[0:index_]\n",
    "        rand_indices = np.random.randint(0,index_-1,64)\n",
    "        train_data = train_data[rand_indices]\n",
    "        train_labels = [task] * len(train_data)\n",
    "        return Variable(torch.FloatTensor(train_data.tolist())), Variable(torch.LongTensor(train_labels), requires_grad = False)\n",
    "    else:\n",
    "        index_ = int(0.8 * n)\n",
    "        rand_indices = np.random.randint(index_,len(task_data),64)\n",
    "        test_data = task_data[rand_indices]\n",
    "        test_labels = [task] * len(test_data)\n",
    "        return Variable(torch.FloatTensor(test_data.tolist())), Variable(torch.LongTensor(test_labels), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen_minibatches(train_dataset_np, train_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_TASKS = [0,2,4,6,8]\n",
    "ALPHA = 0.01\n",
    "BETA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.4386\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.4612\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.6472\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4481\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())\n",
    "task_weights_list = []\n",
    "val_loss_list = []\n",
    "for task in TRAIN_TASKS:\n",
    "    train_img_batch, train_labels_batch = gen_minibatches(train_dataset_np, train_labels, task)\n",
    "    output = mlp(train_img_batch, base_weights)\n",
    "    loss = loss_function(output, train_labels_batch)\n",
    "#     print loss\n",
    "    grad_params = grad(loss, mlp.parameters(), create_graph=True)    \n",
    "    task_weights= OrderedDict((name, param - ALPHA*grad) for ((name, param), grad) in zip(base_weights.items(), grad_params))\n",
    "    task_weights_list.append(task_weights)\n",
    "    # forward pass on validation\n",
    "    val_img_batch, val_labels_batch = gen_minibatches(train_dataset_np, train_labels, task, train=False)\n",
    "    output = mlp(val_img_batch, task_weights)\n",
    "    loss = loss_function(output, val_labels_batch)\n",
    "    val_loss_list.append(loss)\n",
    "    print loss\n",
    "# meta_update\n",
    "meta_grads = OrderedDict({k: sum(d[k] for d in task_weights_list) for k in task_weights_list[0].keys()})\n",
    "meta_loss = sum(val_loss_list)\n",
    "meta_grad_params = grad(meta_loss, mlp.parameters(), create_graph=True)    \n",
    "\n",
    "meta_weights = OrderedDict((name, param - BETA*grad) for ((name, param), grad) in zip(base_weights.items(), meta_grad_params))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outer loss = all_task_loss\n",
    "# outer gradient = summation_on_inner_gradients\n",
    "# outer update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight', Variable containing:\n",
       "              -9.2867e-04  2.3334e-02  1.0286e-02  ...  -2.1942e-02 -4.7197e-04 -3.7485e-04\n",
       "               2.0157e-02 -1.3466e-02 -3.0656e-02  ...  -2.3907e-02 -7.0711e-03  3.0730e-02\n",
       "               5.2343e-03  6.5051e-03 -2.0429e-02  ...   6.8701e-03 -1.8204e-02  1.6440e-02\n",
       "                              ...                   â‹±                   ...                \n",
       "              -8.8931e-03 -1.0550e-02  2.3829e-04  ...  -3.2756e-02  9.2230e-03  1.7846e-02\n",
       "               3.5419e-02 -2.5387e-02  7.2238e-03  ...   6.6081e-04 -1.0161e-03 -1.1414e-03\n",
       "              -2.3213e-02  1.4009e-02  1.8974e-02  ...  -1.1989e-02 -8.0940e-03 -3.1297e-02\n",
       "              [torch.FloatTensor of size 128x784]),\n",
       "             ('layer1.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "               -0.1601\n",
       "               -2.5524\n",
       "                0.9776\n",
       "               -0.5021\n",
       "               -2.9744\n",
       "                1.1096\n",
       "               -1.4821\n",
       "               -1.7987\n",
       "               -0.2626\n",
       "               -3.1931\n",
       "               -0.3760\n",
       "               -3.2748\n",
       "               -0.6268\n",
       "               -1.7856\n",
       "               -2.6216\n",
       "               -1.0307\n",
       "               -3.4513\n",
       "                0.0726\n",
       "               -3.2009\n",
       "                1.7547\n",
       "               -0.8915\n",
       "               -0.4334\n",
       "                2.1277\n",
       "               -1.9818\n",
       "               -0.1018\n",
       "               -0.2294\n",
       "                3.5178\n",
       "                1.0876\n",
       "               -1.5419\n",
       "                0.6928\n",
       "                0.4603\n",
       "               -1.7218\n",
       "               -0.7624\n",
       "               -2.7031\n",
       "               -0.1328\n",
       "               -1.6760\n",
       "                3.5707\n",
       "                0.5707\n",
       "                2.0562\n",
       "                2.4983\n",
       "                3.4904\n",
       "                1.1863\n",
       "                0.7797\n",
       "               -1.1134\n",
       "               -3.5051\n",
       "                1.4347\n",
       "                0.0514\n",
       "                1.2043\n",
       "               -2.4606\n",
       "               -0.6773\n",
       "               -0.5914\n",
       "                0.2073\n",
       "                2.6316\n",
       "               -1.2881\n",
       "               -2.6177\n",
       "               -0.9970\n",
       "               -1.6591\n",
       "               -3.4601\n",
       "                2.1532\n",
       "                0.7216\n",
       "               -1.1614\n",
       "               -0.0700\n",
       "               -3.2675\n",
       "                3.0191\n",
       "                1.8991\n",
       "               -3.3547\n",
       "               -3.0443\n",
       "                0.3875\n",
       "                2.1856\n",
       "                1.5277\n",
       "                0.5945\n",
       "               -0.9381\n",
       "                0.3427\n",
       "                3.0093\n",
       "               -0.2242\n",
       "                2.5143\n",
       "               -0.3876\n",
       "               -2.3403\n",
       "                0.7375\n",
       "                2.3828\n",
       "               -2.4173\n",
       "               -0.7246\n",
       "                2.7174\n",
       "               -3.1961\n",
       "                1.6130\n",
       "                1.1381\n",
       "                2.3407\n",
       "               -3.5625\n",
       "               -1.1110\n",
       "               -1.4060\n",
       "               -0.3811\n",
       "               -1.9080\n",
       "               -3.2516\n",
       "                0.7111\n",
       "                2.5085\n",
       "                0.2055\n",
       "                2.7719\n",
       "               -1.1401\n",
       "                0.2575\n",
       "                3.4403\n",
       "               -1.1906\n",
       "                0.7290\n",
       "                3.0260\n",
       "               -0.7559\n",
       "               -2.9912\n",
       "                1.2979\n",
       "                3.1114\n",
       "                0.9998\n",
       "                1.1996\n",
       "               -2.2063\n",
       "                1.9264\n",
       "               -1.2564\n",
       "               -2.2189\n",
       "               -1.2724\n",
       "                2.0038\n",
       "                0.2039\n",
       "               -1.7732\n",
       "               -2.3320\n",
       "               -0.6216\n",
       "               -2.7872\n",
       "               -0.9968\n",
       "               -1.3627\n",
       "                0.5672\n",
       "               -1.2703\n",
       "               -2.0141\n",
       "                3.4259\n",
       "               -1.6901\n",
       "                3.0181\n",
       "              [torch.FloatTensor of size 128]),\n",
       "             ('layer2.weight', Variable containing:\n",
       "              1.00000e-02 *\n",
       "              -4.0614  7.7641 -7.1004  ...   4.7683  3.7475 -0.6570\n",
       "               1.0742  2.6496  2.0823  ...  -2.4504  4.0128 -5.3936\n",
       "               2.2438  0.8089 -5.9563  ...   1.8516 -4.8014 -7.7822\n",
       "                        ...             â‹±             ...          \n",
       "               6.5694  4.1663  2.1261  ...  -3.7333  6.0316  6.8832\n",
       "               3.3589 -3.0458  2.3448  ...  -0.1695  2.9083  6.0568\n",
       "               3.5675  6.4081 -5.9355  ...  -5.1823 -0.9008  5.3877\n",
       "              [torch.FloatTensor of size 128x128]),\n",
       "             ('layer2.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "                1.7191\n",
       "               -4.3909\n",
       "               -5.0555\n",
       "               -2.2850\n",
       "               -2.4030\n",
       "                4.8830\n",
       "                7.5415\n",
       "                6.5772\n",
       "                5.4200\n",
       "                4.9677\n",
       "               -1.3311\n",
       "               -6.5558\n",
       "                4.3192\n",
       "               -0.9679\n",
       "                8.1504\n",
       "                1.9383\n",
       "                3.0875\n",
       "               -1.5790\n",
       "                8.7485\n",
       "               -1.6194\n",
       "               -6.3120\n",
       "               -3.5505\n",
       "                5.3696\n",
       "               -3.3513\n",
       "               -5.9126\n",
       "                2.3124\n",
       "               -2.6621\n",
       "                3.5189\n",
       "               -7.5131\n",
       "               -3.4519\n",
       "                1.5606\n",
       "               -5.8038\n",
       "               -4.4473\n",
       "               -2.5060\n",
       "                4.3712\n",
       "               -6.8993\n",
       "                1.6882\n",
       "                1.2613\n",
       "                7.0393\n",
       "                8.3322\n",
       "               -2.1652\n",
       "               -8.0913\n",
       "                1.6638\n",
       "               -8.3034\n",
       "                2.4667\n",
       "                1.2783\n",
       "               -0.9395\n",
       "                0.1996\n",
       "                4.2804\n",
       "               -2.5154\n",
       "               -4.3988\n",
       "               -0.3079\n",
       "                2.6737\n",
       "                6.8989\n",
       "                1.3059\n",
       "               -1.2025\n",
       "                4.7187\n",
       "                1.9321\n",
       "                0.6039\n",
       "               -4.7755\n",
       "                2.0023\n",
       "               -1.1723\n",
       "                1.2863\n",
       "               -6.4668\n",
       "               -3.6353\n",
       "                8.0052\n",
       "                3.3278\n",
       "                3.6847\n",
       "                1.3460\n",
       "               -4.5117\n",
       "                4.4790\n",
       "               -3.6923\n",
       "               -8.1720\n",
       "               -3.3261\n",
       "               -5.3185\n",
       "                4.7470\n",
       "               -3.6182\n",
       "                7.1745\n",
       "               -7.8843\n",
       "                0.7630\n",
       "                7.9860\n",
       "                2.4130\n",
       "                6.0431\n",
       "                1.1875\n",
       "                5.9226\n",
       "                8.3004\n",
       "               -1.2053\n",
       "                8.0694\n",
       "               -7.1366\n",
       "               -0.7547\n",
       "                4.6273\n",
       "                2.0237\n",
       "               -2.6117\n",
       "               -0.1265\n",
       "                5.6368\n",
       "               -3.2981\n",
       "                6.8086\n",
       "                8.4038\n",
       "               -4.8052\n",
       "                0.2134\n",
       "               -2.1237\n",
       "               -7.4211\n",
       "                4.3823\n",
       "               -1.5409\n",
       "               -1.0747\n",
       "               -5.7010\n",
       "               -4.9106\n",
       "               -3.9119\n",
       "               -3.2719\n",
       "               -1.8068\n",
       "               -5.9303\n",
       "                2.9691\n",
       "               -5.2914\n",
       "                7.6053\n",
       "               -4.8756\n",
       "                4.4197\n",
       "                6.9702\n",
       "               -1.9854\n",
       "               -3.8918\n",
       "               -6.0691\n",
       "                5.9536\n",
       "               -5.2971\n",
       "               -7.5734\n",
       "                3.5024\n",
       "               -8.8227\n",
       "                3.7512\n",
       "               -5.8152\n",
       "                6.0295\n",
       "              [torch.FloatTensor of size 128]),\n",
       "             ('output_layer.weight', Variable containing:\n",
       "              -0.0469 -0.0629 -0.0393  ...   0.0391  0.0161  0.0172\n",
       "              -0.0782  0.0251 -0.0475  ...   0.0732 -0.0267  0.0299\n",
       "               0.0075  0.0649  0.0724  ...   0.0645 -0.0233 -0.0875\n",
       "                        ...             â‹±             ...          \n",
       "              -0.0888  0.0417  0.0131  ...   0.0387  0.0256 -0.0527\n",
       "              -0.0050  0.0819  0.0201  ...   0.0095 -0.0485  0.0074\n",
       "              -0.0242 -0.0168 -0.0506  ...  -0.0746 -0.0791 -0.0648\n",
       "              [torch.FloatTensor of size 10x128]),\n",
       "             ('output_layer.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "               -0.4161\n",
       "               -1.1372\n",
       "               -7.7133\n",
       "               -0.9618\n",
       "                2.0981\n",
       "               -4.0902\n",
       "               -2.9724\n",
       "                5.2587\n",
       "               -4.9219\n",
       "                4.0887\n",
       "              [torch.FloatTensor of size 10])])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
