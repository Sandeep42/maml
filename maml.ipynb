{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('mnist', download=False, train=True, transform=ToTensor())\n",
    "test_dataset = datasets.MNIST('mnist', download=False, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, layer_sizes=[128,128]):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer1_size = layer_sizes[0]\n",
    "        self.layer2_size = layer_sizes[1]\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, layer_sizes[0])\n",
    "        self.layer2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.output_layer = nn.Linear(layer_sizes[1], output_size)\n",
    "        \n",
    "    def forward(self, input_batch, weights=None):        \n",
    "        x = F.relu(F.linear(input_batch,weights['layer1.weight'],weights['layer1.bias']))\n",
    "        x = F.relu(F.linear(x,weights['layer2.weight'],weights['layer2.bias']))\n",
    "        output = F.linear(x,weights['output_layer.weight'],weights['output_layer.bias'])\n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SimpleMLP(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mlp(Variable(torch.randn(64,784)), base_weights)\n",
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on 0, 2, 4, 6, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meta train on 1, 3, 5, 7, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset.train_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_np = train_dataset_np.reshape((train_dataset_np.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels  = np.array(train_dataset.train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_minibatches(data, labels, task, train=True):\n",
    "    task_indices = np.where(labels == task)[0]\n",
    "    task_data = data[task_indices]\n",
    "    n = len(task_data)\n",
    "    if train:\n",
    "        index_ = int(0.8 * n)\n",
    "        train_data = task_data[0:index_]\n",
    "        train_labels = labels[0:index_]\n",
    "        rand_indices = np.random.randint(0,index_-1,64)\n",
    "        train_data = train_data[rand_indices]\n",
    "        train_labels = [task] * len(train_data)\n",
    "        return Variable(torch.FloatTensor(train_data.tolist())), Variable(torch.LongTensor(train_labels), requires_grad = False)\n",
    "    else:\n",
    "        index_ = int(0.8 * n)\n",
    "        rand_indices = np.random.randint(index_,len(task_data),64)\n",
    "        test_data = task_data[rand_indices]\n",
    "        test_labels = [task] * len(test_data)\n",
    "        return Variable(torch.FloatTensor(test_data.tolist())), Variable(torch.LongTensor(test_labels), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen_minibatches(train_dataset_np, train_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_TASKS = [0,2,4,6,8]\n",
    "ALPHA = 0.01\n",
    "BETA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.4612\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4422\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4479\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 2.4410\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while i<=100:\n",
    "    base_weights = OrderedDict((name, param) for (name, param) in mlp.named_parameters())\n",
    "    task_weights_list = []\n",
    "    val_loss_list = []\n",
    "    for task in TRAIN_TASKS:\n",
    "        train_img_batch, train_labels_batch = gen_minibatches(train_dataset_np, train_labels, task)\n",
    "        output = mlp(train_img_batch, base_weights)\n",
    "        loss = loss_function(output, train_labels_batch)\n",
    "    #     print loss\n",
    "        grad_params = grad(loss, mlp.parameters(), create_graph=True)    \n",
    "        task_weights= OrderedDict((name, param - ALPHA*grad) for ((name, param), grad) in zip(base_weights.items(), grad_params))\n",
    "        task_weights_list.append(task_weights)\n",
    "        # forward pass on validation\n",
    "        val_img_batch, val_labels_batch = gen_minibatches(train_dataset_np, train_labels, task, train=False)\n",
    "        output = mlp(val_img_batch, task_weights)\n",
    "        loss = loss_function(output, val_labels_batch)\n",
    "        val_loss_list.append(loss)\n",
    "        print loss\n",
    "    # meta_update\n",
    "    meta_grads = OrderedDict({k: sum(d[k] for d in task_weights_list) for k in task_weights_list[0].keys()})\n",
    "    meta_loss = sum(val_loss_list)\n",
    "    meta_grad_params = grad(meta_loss, mlp.parameters(), create_graph=True)    \n",
    "    meta_weights = OrderedDict((name, param - BETA*grad) for ((name, param), grad) in zip(base_weights.items(), meta_grad_params))\n",
    "    ml\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outer loss = all_task_loss\n",
    "# outer gradient = summation_on_inner_gradients\n",
    "# outer update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight', Variable containing:\n",
       "               6.4644e-04 -3.4100e-02 -2.5517e-03  ...   1.9912e-02  2.8152e-02 -6.1706e-03\n",
       "               1.9021e-02  3.4480e-02 -1.9157e-02  ...  -2.4794e-02  1.8581e-03 -3.8926e-03\n",
       "              -1.5606e-02  3.4159e-02  2.3828e-02  ...   1.7006e-03  2.3790e-02 -3.1260e-02\n",
       "                              ...                   â‹±                   ...                \n",
       "               3.2716e-02  2.8180e-02 -2.9216e-02  ...   1.4360e-02 -3.3366e-02  4.8171e-03\n",
       "              -3.5637e-02  7.1041e-03  8.8678e-03  ...   3.5618e-02  1.0965e-02  6.9535e-03\n",
       "               3.9167e-03  2.2292e-02 -3.2668e-02  ...   3.0164e-02 -1.1941e-02 -1.3808e-02\n",
       "              [torch.FloatTensor of size 128x784]),\n",
       "             ('layer1.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "                0.0830\n",
       "               -3.0036\n",
       "               -1.0059\n",
       "               -0.6468\n",
       "                3.5578\n",
       "               -3.4751\n",
       "               -2.0600\n",
       "                2.6817\n",
       "                1.5692\n",
       "                3.4080\n",
       "               -1.6980\n",
       "                0.3222\n",
       "                1.9959\n",
       "                1.4574\n",
       "                2.9072\n",
       "               -3.0184\n",
       "                2.5652\n",
       "               -0.2677\n",
       "               -1.7902\n",
       "                0.0729\n",
       "               -1.9264\n",
       "                3.4946\n",
       "               -3.0548\n",
       "                0.5368\n",
       "                2.7271\n",
       "                0.7596\n",
       "               -0.4348\n",
       "                1.1020\n",
       "                2.3158\n",
       "               -2.3897\n",
       "               -0.8117\n",
       "                2.2421\n",
       "                2.3011\n",
       "               -3.0062\n",
       "               -1.3324\n",
       "                0.2403\n",
       "               -0.0807\n",
       "               -2.5223\n",
       "               -1.6903\n",
       "                1.9880\n",
       "                2.2086\n",
       "                0.4764\n",
       "                1.8708\n",
       "               -1.5311\n",
       "               -0.0601\n",
       "               -3.3013\n",
       "               -0.3350\n",
       "                0.4222\n",
       "                2.1875\n",
       "                0.7500\n",
       "                0.0329\n",
       "                1.7437\n",
       "               -2.9810\n",
       "                1.0820\n",
       "                0.4801\n",
       "                1.3856\n",
       "               -2.2359\n",
       "               -0.7014\n",
       "               -3.0451\n",
       "                1.0684\n",
       "                1.7932\n",
       "               -1.8847\n",
       "                1.6504\n",
       "               -0.5411\n",
       "               -1.2135\n",
       "                1.2614\n",
       "               -0.2858\n",
       "               -3.1272\n",
       "               -1.2724\n",
       "               -1.7128\n",
       "               -0.8341\n",
       "                1.3517\n",
       "               -0.8020\n",
       "                3.3309\n",
       "                1.3312\n",
       "               -0.0979\n",
       "               -2.7764\n",
       "               -3.0798\n",
       "                1.2053\n",
       "                3.5134\n",
       "               -1.8258\n",
       "               -0.0715\n",
       "                1.5745\n",
       "               -0.8081\n",
       "               -2.0884\n",
       "               -0.3665\n",
       "               -1.4164\n",
       "               -1.2128\n",
       "               -1.7449\n",
       "                2.0732\n",
       "               -0.9166\n",
       "               -2.1859\n",
       "                1.5411\n",
       "                0.6911\n",
       "                1.7417\n",
       "               -1.4860\n",
       "                1.5163\n",
       "               -2.6304\n",
       "               -2.2840\n",
       "                0.8657\n",
       "               -1.3994\n",
       "               -1.8946\n",
       "               -1.7750\n",
       "               -3.2888\n",
       "                2.2324\n",
       "                0.5691\n",
       "               -0.1047\n",
       "                0.9267\n",
       "                3.4663\n",
       "               -2.4280\n",
       "                2.2322\n",
       "                2.4912\n",
       "                0.1577\n",
       "                2.6200\n",
       "               -0.8195\n",
       "                0.7800\n",
       "                0.6833\n",
       "               -1.1906\n",
       "                1.0897\n",
       "               -2.3808\n",
       "                1.0652\n",
       "               -0.3078\n",
       "               -3.2908\n",
       "                0.6053\n",
       "                1.9338\n",
       "                2.2715\n",
       "               -1.1266\n",
       "               -1.5483\n",
       "              [torch.FloatTensor of size 128]),\n",
       "             ('layer2.weight', Variable containing:\n",
       "              1.00000e-02 *\n",
       "               6.9480  5.4346 -3.4277  ...  -5.7209 -3.0694  4.7037\n",
       "               3.1252  2.4784 -7.7323  ...   2.3675  0.9222 -6.9340\n",
       "              -6.5361 -4.3069  7.9201  ...  -7.8460 -0.8863  2.9665\n",
       "                        ...             â‹±             ...          \n",
       "               8.4054  0.9079 -1.9642  ...   1.0110 -5.7692  5.4646\n",
       "               8.7916 -6.1113  7.0864  ...   1.8260 -7.1153  7.3184\n",
       "               2.6434 -6.5554 -4.2478  ...  -5.7504  5.7120  6.6109\n",
       "              [torch.FloatTensor of size 128x128]),\n",
       "             ('layer2.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "                1.5089\n",
       "               -8.7485\n",
       "               -7.0272\n",
       "                5.0208\n",
       "                1.3217\n",
       "               -1.4226\n",
       "               -4.4657\n",
       "                7.2171\n",
       "                4.5057\n",
       "                2.2705\n",
       "               -2.5385\n",
       "               -7.9151\n",
       "               -7.6681\n",
       "                0.0950\n",
       "                8.1894\n",
       "               -5.7725\n",
       "               -1.7501\n",
       "               -8.4979\n",
       "               -6.6685\n",
       "               -2.2182\n",
       "               -6.1211\n",
       "               -3.2859\n",
       "               -4.3419\n",
       "               -0.8827\n",
       "                0.8834\n",
       "                1.6797\n",
       "                6.3255\n",
       "                3.6998\n",
       "                2.7790\n",
       "                3.5491\n",
       "                4.3541\n",
       "                3.9773\n",
       "               -4.6782\n",
       "                6.9617\n",
       "                4.3997\n",
       "               -7.9900\n",
       "               -1.7367\n",
       "                2.4955\n",
       "                4.8045\n",
       "               -5.0665\n",
       "                7.9285\n",
       "                8.6994\n",
       "                5.1936\n",
       "               -2.9092\n",
       "                1.1847\n",
       "                3.7426\n",
       "                0.3985\n",
       "               -5.1200\n",
       "               -3.4487\n",
       "               -7.9693\n",
       "               -0.8966\n",
       "                0.4249\n",
       "               -8.3230\n",
       "                6.4089\n",
       "                3.7299\n",
       "               -0.3484\n",
       "               -4.4576\n",
       "               -4.9552\n",
       "               -5.9790\n",
       "               -1.5068\n",
       "               -5.1032\n",
       "                6.4464\n",
       "                1.9512\n",
       "                4.9824\n",
       "                5.0865\n",
       "                1.5501\n",
       "               -4.8928\n",
       "                1.5062\n",
       "                2.1857\n",
       "               -1.3287\n",
       "                6.4142\n",
       "                5.4273\n",
       "                0.7749\n",
       "               -7.0493\n",
       "               -8.6902\n",
       "               -6.7155\n",
       "                8.5335\n",
       "               -5.5388\n",
       "                0.9877\n",
       "               -7.8392\n",
       "                3.3873\n",
       "                1.5853\n",
       "               -6.7612\n",
       "               -1.4429\n",
       "                6.4159\n",
       "                8.3042\n",
       "               -4.9337\n",
       "                6.7208\n",
       "               -1.7616\n",
       "               -2.0455\n",
       "               -1.4005\n",
       "                1.0857\n",
       "               -2.3618\n",
       "               -8.2895\n",
       "               -0.8797\n",
       "                2.1329\n",
       "                7.3716\n",
       "                6.4978\n",
       "               -3.5490\n",
       "               -6.6761\n",
       "                2.0861\n",
       "               -0.3992\n",
       "                7.6105\n",
       "                4.4190\n",
       "                0.0263\n",
       "               -4.5670\n",
       "                7.3722\n",
       "                0.7947\n",
       "                3.4450\n",
       "                3.0205\n",
       "                3.8753\n",
       "               -4.1324\n",
       "               -1.3646\n",
       "               -5.2023\n",
       "                1.7934\n",
       "               -3.5816\n",
       "               -4.2861\n",
       "                6.3398\n",
       "               -4.7127\n",
       "                4.8149\n",
       "               -2.1764\n",
       "               -7.5892\n",
       "               -0.8468\n",
       "                4.3270\n",
       "                3.6897\n",
       "                4.1439\n",
       "                5.6553\n",
       "               -3.6246\n",
       "              [torch.FloatTensor of size 128]),\n",
       "             ('output_layer.weight', Variable containing:\n",
       "              1.00000e-02 *\n",
       "              -8.8277 -7.6838 -8.2015  ...  -3.3220  2.6469 -7.1443\n",
       "               6.0765 -4.1015  6.5805  ...  -5.4264  7.1511  5.0674\n",
       "               1.1721 -1.5940 -2.8930  ...  -3.6497  6.4237  1.5312\n",
       "                        ...             â‹±             ...          \n",
       "               6.4421  8.4701  0.0864  ...  -6.9478 -2.0073 -3.8811\n",
       "              -6.9020 -3.8882  1.6237  ...   4.7136  0.1439  6.1850\n",
       "               3.7946 -5.2352 -0.6175  ...   8.1926 -7.9882  6.9006\n",
       "              [torch.FloatTensor of size 10x128]),\n",
       "             ('output_layer.bias', Variable containing:\n",
       "              1.00000e-02 *\n",
       "               -6.2140\n",
       "                4.6332\n",
       "                4.9875\n",
       "                7.5529\n",
       "               -6.0410\n",
       "               -1.4181\n",
       "                0.2318\n",
       "                4.1493\n",
       "                4.9464\n",
       "               -2.6092\n",
       "              [torch.FloatTensor of size 10])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
